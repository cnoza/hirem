---
title: "Testing the hierarchical reserving models on the simulation engine proposed by Wang and Wuthrich"
author: "Christophe Nozaradan"
date: "October 2022"
output: html_document
bibliography: references.bib  
---

```{css, echo=FALSE}
.main-container {
  max-width: 1200px; 
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
options(dplyr.summarise.inform = FALSE)
```

## Introduction
This document implements hierarchical reserving models based on the extended hirem package. The input data is generated using the simulation engine from Wang and Wuthrich, described the following paper:

Wang, Melantha and Wuthrich, Mario V., Individual Claims Generator for Claims Reserving Studies: Data Simulation.R (June 3, 2022). Available at SSRN: https://ssrn.com/abstract=4127073 or http://dx.doi.org/10.2139/ssrn.4127073

## Simulating the portfolio
We evaluate the predictive performance of our hierarchical reserving model (hierarchical GLM, hierarchical GBM, hierarchical XGB and hierarchical DNN) on one portfolio simulated along the extreme event scenario. We compare the results with the ones obtained using an aggregate reserving model, namely the classical chain ladder model.

First, we load the required packages.

```{r, results='hide'}
require(SynthETIC)  # 1.0.0 or higher needed
require(plyr)
require(dplyr)
require(locfit)
require(actuar)
require(tidyr)
require(lubridate)
require(tidyverse)
require(xgboost)
require(Matrix)
require(tidyverse)
require(data.table)
require(ggplot2)
require(recipes)
require(tensorflow)
require(keras)
#install.packages('devtools')
#devtools::install_github('cnoza/hirem')
require(hirem)

```

We simulate one portfolio using the simulation engine from Wang and Wuthrich.

The seed number is:
```{r}
cat(sprintf("%s", seed))
```


```{r}
ref_claim <- 1          # currency is 1
time_unit <- 1/12       # we consider monthly claims development
set_parameters(ref_claim=ref_claim, time_unit=time_unit)

years <- 17               # number of occurrence years
I <- years/time_unit      # number of development periods

source("./wang-wuthrich/Tools/functions simulation.R")
dest_path <- "" # plot to be saved in ...

#####################################################################
#### generate claim data
#####################################################################

# generate individual claims: may take 60 seconds
claims_list <- data.generation(seed=seed, future_info=F)

# We unset the seed
set.seed(NULL)

# save individual output for easier access
claims <- claims_list[[1]]
claims <- claims %>% filter(Status == 'Closed')

paid <- claims_list[[2]]

# We now build a reserving dataset compatible with hirem

d0 <- merge(paid, claims, by="Id") %>%
  arrange(Id, EventId)

d0 <- d0 %>% 
  mutate(calendar.year = floor(EventMonth/12)+1,
         rep.year = floor(RepMonth/12)+1,
         dev.year = calendar.year - rep.year + 1,
         size = Paid) %>%
  select(Id, dev.year, size)

d1 <- expand.grid(Id = unique(d0$Id),
                  dev.year = 1:max(d0$dev.year),
                  size = 0)

d2 <- union_all(d0,d1)

d3 <- d2 %>% 
    group_by(Id, dev.year) %>%
    dplyr::summarise(total = sum(size))
colnames(d3) <- c('Id','dev.year', 'size')

d4 <- merge(d3, claims, by="Id", all.x = T)

reserving_data <- d4 %>% 
  mutate(rep.year = floor(RepMonth/12)+1,
         calendar.year = dev.year + rep.year - 1,
         settlement.year = floor(SetMonth/12)+1,
         payment = as.integer(size>0),
         recovery = as.integer(size<0),
         size.pay = ifelse(size>0,size,0),
         size.recov = ifelse(size<0, -size, 0),
         occ.month = AccMonth,
         occ.date = AccDate,
         occ.weekday = factor(AccWeekday),
         rep.date = RepDate,
         age = Age,
         age.fact = factor(Age),
         settlement = as.integer(calendar.year >= settlement.year),
         open = as.integer(ifelse(calendar.year==settlement.year,1,1-settlement)),
         type = factor(Type),
         rep.month = factor(month(ymd(RepDate)))) %>%
  select(-CumPaid, -Type, -Ultimate, -Status, -PayCount, -AccMonth, -AccDate, -Age, -AccWeekday, -RepDate, RepMonth) %>%
  arrange(Id, dev.year)

reserving_data <- reserving_data %>% filter(rep.year <= 10 & dev.year <= 10)

ultimate <- reserving_data %>% 
  dplyr::group_by(Id) %>% 
  dplyr::summarise(total.size.pay = sum(size.pay))

reserving_data <- merge(reserving_data, ultimate, by="Id", all.x=T)

reserving_data <- reserving_data %>%
  mutate(p.size.recov = ifelse(size<0, -size/total.size.pay, 0),
         q.size.recov = ifelse(size<0, p.size.recov/(1-p.size.recov), 0))

reserving_data$dev.year.fact <- factor(reserving_data$dev.year, levels = 1:max(reserving_data$dev.year))
reserving_data$rep.year.fact <- factor(reserving_data$rep.year, levels = 1:max(reserving_data$rep.year))

reserving_data$monthDev12 <- as.character(reserving_data$rep.month)
reserving_data$monthDev12[reserving_data$dev.year > 3] <- 'dev.year.over.3'
reserving_data$devYearMonth <- factor(paste(reserving_data$dev.year, reserving_data$monthDev12, sep = '-'))

str(reserving_data)

```
 
Next, we set the evaluation date to .... The observed portfolio then consists of the claims that are reported before this evaluation date, i.e. with calendar year before year 9.  The prediction data set consists of the remaining claims, namely the claims reported after calendar year 9.

```{r}
# Observed and prediction data set
observed_data   <- reserving_data %>% filter(calendar.year <= 10)
prediction_data <- reserving_data %>% filter(calendar.year > 10)
```

## Calibration of the hierarhical reserving models
We first define the weights used in the calibration of the hierarchical reserving model. The purpose of these weights is to replicate the development year distribution from the prediction data set to the observed data set (training data set). Hence, more weight is assigned to claims observations in later development years since reporting. We give a very small weight to the claims observations in the first development year since reporting because the prediction data set does not contain observations for this development year. More details are provided in @hirempaper

```{r}
# Calculating the weights
reported_claims <- observed_data %>%
  dplyr::filter(dev.year == 1) %>%
  group_by(rep.year) %>% 
  dplyr::summarise(count = n()) %>%
  pull(count)

denominator <- tail(rev(cumsum(reported_claims)), -1)
numerator <- head(cumsum(rev(reported_claims)), -1)
weight <- c(10^(-6), numerator / denominator)

names(weight) <- paste0('dev.year',1:10)
weight
```

### Hierarchical GLM

The hierarchical GLM consists of the three layer structure: settlement, payment and size. We model the settlement indicator using a binomial regression model with a complementary log link function. The payment indicator will be modeled using a logistic regression model and the payment sizes using a Gamma GLM with log-link function. 

We define the model specifications of each layer. We only train the reserving model on the claim updates that are still open at the beginning of each year and are observed on the evaluation date (`calendar.year <= 10`).

```{r}
# Model specifications hierarchical GLM
model_glm  <- hirem(reserving_data) %>%
  split_data(observed = function(df) df %>% filter(calendar.year <= 10, open == 1)) %>%
  layer_glm(name = 'settlement', 'family' = binomial(link = cloglog)) %>%
  layer_glm(name = 'payment', 'family' = binomial(link = logit)) %>%
  layer_glm(name = 'recovery', 'family' = binomial(link = logit)) %>%
  layer_glm(name = 'size.pay', 'family' = Gamma(link = 'log'), 
            filter = function(x){x$payment == 1}) %>%
  layer_glm(name = 'q.size.recov', 'family' = Gamma(link = 'log'), 
            filter = function(x){x$recovery == 1})
```


```{r}
# Covariate selection function
covariate_selection <- function(response_variable, all_covariates, family, data, weight, minimize="deviance") {
  aic_sav <- NULL
  deviance_sav <- NULL
  aic <- c()
  deviance <- c()
  formula_resp_list <- c()
  step <- c()
  i = 1
  covariate_sav_list <- c()
  weights.vec <- weight[data[['dev.year']]]
  
  while(length(all_covariates) > 0) {
    covariate_sav <- NULL
    for(covariate in all_covariates) {
      cov_list <- append(covariate_sav_list, covariate)
      formula_resp <- as.formula(paste0(response_variable, ' ~ ',paste0(cov_list, collapse = ' + ')))
      m_glm <- glm(formula_resp , family = family, data = data, weights = weights.vec)
      aic <- append(aic, m_glm$aic)
      deviance <- append(deviance, m_glm$deviance)
      formula_resp_list <- append(formula_resp_list, formula_resp)
      step = append(step,i)
      
      if(is.null(aic_sav)) aic_sav = m_glm$aic + 1
      if(is.null(deviance_sav)) deviance_sav = m_glm$deviance + 1
      
      if( (m_glm$deviance < deviance_sav & minimize == "deviance") 
          | (m_glm$aic < aic_sav & minimize == "aic")) {
        #formula_resp_sav <- formula_resp
        covariate_sav <- covariate
        aic_sav <- m_glm$aic
        deviance_sav <- m_glm$deviance
      }
    }
    if(!is.null(covariate_sav)) {
      covariate_sav_list <- append(covariate_sav_list, covariate_sav)
      all_covariates <- all_covariates[all_covariates != covariate_sav]
      i=i+1
    } else all_covariates <- NULL
    
  }
  
  if(minimize=="deviance")
    res.tab <- data.frame(step = step, 
                          m_formula = as.character(formula_resp_list), 
                          aic = aic, 
                          deviance = deviance) %>% arrange(deviance)
  else
    res.tab <- data.frame(step = step, 
                          m_formula = as.character(formula_resp_list), 
                          aic = aic, deviance = deviance) %>% arrange(aic)
  
  return(res.tab)
  
}

# Covariate selection for settlement layer
all_covariates <- c('type', 'dev.year.fact', 'rep.month', 'devYearMonth', 'age.fact')
family.settle <- model_glm$layers$settlement$method_options
res.tab.settle <- covariate_selection('settlement', all_covariates, family.settle, 
                                      model_glm$data_observed, weight, minimize='deviance')
print(res.tab.settle)

# Covariate selection for payment layer
all_covariates <- c('settlement', 'type', 'dev.year.fact', 'rep.month', 'devYearMonth', 'age.fact')
family.pay <- model_glm$layers$payment$method_options
res.tab.pay <- covariate_selection('payment',all_covariates, family.pay, 
                                   model_glm$data_observed, weight, minimize='deviance')
print(res.tab.pay)

# Covariate selection for recovery layer
all_covariates <- c('settlement', 'type', 'dev.year.fact', 'rep.month', 'devYearMonth', 'age.fact')
family.recov <- model_glm$layers$recovery$method_options
res.tab.recov <- covariate_selection('recovery',all_covariates, family.recov, model_glm$data_observed, weight, minimize='deviance')
print(res.tab.recov)

# Covariate selection for size.pay layer
all_covariates <- c('settlement', 'type', 'dev.year.fact', 'rep.month', 'devYearMonth', 'age.fact')
d_size_pay <- model_glm$data_observed %>% filter(payment == 1)
family.size.pay <- model_glm$layers$size.pay$method_options
res.tab.size.pay <- covariate_selection('size.pay', all_covariates, family.size.pay, d_size_pay, weight, minimize='deviance')
print(res.tab.size.pay)

# Covariate selection for size.recov layer
all_covariates <- c('settlement', 'type', 'dev.year.fact', 'rep.month', 'devYearMonth', 'age.fact')
d_size_recov <- model_glm$data_observed %>% filter(recovery == 1)
family.size.recov <- model_glm$layers$q.size.recov$method_options
res.tab.size.recov <- covariate_selection('q.size.recov', all_covariates, family.size.recov, d_size_recov, weight, minimize='deviance')
print(res.tab.size.recov)

```


Next, we provide the formulae for each layer, coming from the covariate selection procedure.


```{r}
# Formulae hierarchical GLM layers - model calibration
formula_settle_glm <- res.tab.settle$m_formula[[1]]
formula_pay_glm    <- res.tab.pay$m_formula[[1]]
formula_recov_glm    <- res.tab.recov$m_formula[[1]]
formula_size_pay_glm   <- res.tab.size.pay$m_formula[[1]]
formula_size_recov_glm    <- res.tab.size.recov$m_formula[[1]]
```

We fit the hierarchical GLM on the observed portfolio of simulated claims.

```{r}
# Fitting the hierarchical GLM
model_glm <- hirem::fit(model_glm,
                 weights = weight,
                 weight.var = 'dev.year',
                 balance.var = 'dev.year',
                 settlement = formula_settle_glm,
                 payment = formula_pay_glm,
                 recovery = formula_recov_glm,
                 size.pay = formula_size_pay_glm,
                 q.size.recov = formula_size_recov_glm)
```

### Hierarchical GBM

The hierarchical GBM consists of the same three layer structure and the same distributional assumptions as the hierarchical GLM. However each layer is now modeled with a GBM instead of a GLM.

We first define the model specifications of each layer. We only train the reserving model on the claim updates that are still open at the beginning of each year and are observed on the evaluation date (`calendar.year <= 10`). We tune some of the GBM parameters (number of trees, interaction depth and shrinkage) using a 5-fold cross validation approach [@hirempaper]. We list here the obtained results from the tuning strategy in the paper. We further fix the `bag.fraction` to 0.75 and the minimum number of observecations each node (`n.minobsinnode`) to 100.

```{r}
# Results of hierarchical model calibration
gbm_param_settle <- list('n.trees' = 225, 'interaction.depth' = 1, 'shrinkage' = 0.05)
gbm_param_pay    <- list('n.trees' = 125, 'interaction.depth' = 3, 'shrinkage' = 0.05)
gbm_param_recov    <- list('n.trees' = 125, 'interaction.depth' = 3, 'shrinkage' = 0.05)
gbm_param_size_pay   <- list('n.trees' = 700, 'interaction.depth' = 1, 'shrinkage' = 0.05)
gbm_param_size_recov   <- list('n.trees' = 700, 'interaction.depth' = 1, 'shrinkage' = 0.05)

# Model specifications
model_gbm <- hirem(reserving_data) %>%
  split_data(observed = function(df) df %>% filter(calendar.year <= 10, open == 1)) %>%
  layer_gbm('settlement', distribution = 'bernoulli', bag.fraction = 0.75, n.minobsinnode = 100,
            n.trees = gbm_param_settle$n.trees, interaction.depth = gbm_param_settle$interaction.depth,
            shrinkage = gbm_param_settle$shrinkage, select_trees = 'last') %>%
  layer_gbm('payment', distribution = 'bernoulli', bag.fraction = 0.75, n.minobsinnode = 100,            
            n.trees = gbm_param_pay$n.trees, interaction.depth = gbm_param_pay$interaction.depth,
            shrinkage = gbm_param_pay$shrinkage, select_trees = 'last') %>%
  layer_gbm('recovery', distribution = 'bernoulli', bag.fraction = 0.75, n.minobsinnode = 100,            
            n.trees = gbm_param_recov$n.trees, interaction.depth = gbm_param_recov$interaction.depth,
            shrinkage = gbm_param_recov$shrinkage, select_trees = 'last') %>%
  layer_gbm('size.pay', distribution = 'gamma', bag.fraction = 0.75, n.minobsinnode = 100,
            n.trees = gbm_param_size_pay$n.trees, interaction.depth = gbm_param_size_pay$interaction.depth,
            shrinkage = gbm_param_size_pay$shrinkage, select_trees = 'last',
            filter = function(data){data$payment == 1}) %>%
  layer_gbm('q.size.recov', distribution = 'gamma', bag.fraction = 0.75, n.minobsinnode = 100,
            n.trees = gbm_param_size_recov$n.trees, interaction.depth = gbm_param_size_recov$interaction.depth,
            shrinkage = gbm_param_size_recov$shrinkage, select_trees = 'last',
            filter = function(data){data$recovery == 1})
```

We now fit the hierarchical GBM on the observed portfolio of simulated claims.

```{r}
# Covariates
covariates_gbm <- c('type', 'dev.year.fact', 'rep.month', 'rep.year.fact', 'age.fact', 'calendar.year')

# Fitting the hierarchical GBM
model_gbm <- hirem::fit(model_gbm,
                 weights = weight,
                 weight.var = 'dev.year',
                 balance.var = 'dev.year',
                 settlement = paste0('settlement ~ 1 + ', paste0(covariates_gbm, collapse = ' + ')),
                 payment = paste0('payment ~ 1 + ', paste0(c(covariates_gbm, 'settlement'), collapse = ' + ')),
                 recovery = paste0('recovery ~ 1 + ', paste0(c(covariates_gbm, 'settlement'), collapse = ' + ')),
                 size.pay = paste0('size.pay ~ 1 + ', paste0(c(covariates_gbm,'settlement'), collapse = ' + ')),
                 q.size.recov = paste0('q.size.recov ~ 1 + ', paste0(c(covariates_gbm,'settlement'), collapse = ' + ')))
```

### Hierarchical XGB

We first define the model specifications of each layer. We only train the reserving model on the claim updates that are still open at the beginning of each year and are observed on the evaluation date (`calendar.year <= 10`).

To tune the hyperparameters, one can activate a cross-validation by setting the `nfolds` parameter to the desired integer in the `layer_xgb` function. Optionally, one can provide a `hyper_grid` parameter or use a predefined one. The hyperparameters performing the best will then be retained automatically.

```{r message=TRUE}
# Results of hierarchical model calibration
xgb_param_settle       <- list('eta' =   .5, 'max_depth' = 3, 'nrounds' =  40, 'subsample' = 1)
xgb_param_pay          <- list('eta' =  .5, 'max_depth' = 3, 'nrounds' =  25, 'subsample' = 1)
xgb_param_recov        <- list('eta' =  .5, 'max_depth' = 3, 'nrounds' =  25, 'subsample' = 1)
xgb_param_size_pay     <- list('eta' = .05, 'max_depth' = 3, 'nrounds' = 800, 'subsample' = 1)
xgb_param_size_recov   <- list('eta' = .05, 'max_depth' = 3, 'nrounds' = 800, 'subsample' = 1)

# Model specifications
model_xgb <- hirem(reserving_data) %>%
  split_data(observed = function(df) df %>% filter(calendar.year <= 10, open == 1)) %>%
  layer_xgb('settlement', objective = 'binary:logistic', eval_metric = 'logloss', grow_policy = 'lossguide',
            eta = xgb_param_settle$eta, max_depth = xgb_param_settle$max_depth, 
            nrounds = xgb_param_settle$nrounds, subsample = xgb_param_settle$subsample) %>%
  layer_xgb('payment', objective = 'binary:logistic', eval_metric = 'logloss', grow_policy = 'lossguide',
            eta = xgb_param_pay$eta, max_depth = xgb_param_pay$max_depth, 
            nrounds = xgb_param_pay$nrounds, subsample = xgb_param_pay$subsample) %>%
  layer_xgb('recovery', objective = 'binary:logistic', eval_metric = 'logloss', grow_policy = 'lossguide',
            eta = xgb_param_recov$eta, max_depth = xgb_param_recov$max_depth, 
            nrounds = xgb_param_recov$nrounds, subsample = xgb_param_recov$subsample) %>%
  layer_xgb('size.pay', objective = 'reg:gamma', 
            eval_metric = 'gamma-deviance', grow_policy = 'lossguide',
            eta = xgb_param_size_pay$eta, max_depth = xgb_param_size_pay$max_depth, 
            nrounds = xgb_param_size_pay$nrounds, subsample = xgb_param_size_pay$subsample,
            filter = function(data){data$payment == 1}) %>%
  layer_xgb('q.size.recov', objective = 'reg:gamma', 
            eval_metric = 'gamma-deviance', grow_policy = 'lossguide',
            eta = xgb_param_size_recov$eta, max_depth = xgb_param_size_recov$max_depth, 
            nrounds = xgb_param_size_recov$nrounds, subsample = xgb_param_size_recov$subsample,
            filter = function(data){data$recovery == 1})
```

We now fit the hierarchical XGB on the observed portfolio of simulated claims.

```{r}
# Covariates
covariates_xgb <- c('type', 'dev.year.fact', 'rep.month', 'rep.year.fact', 'age.fact', 'calendar.year')

# Fitting the hierarchical XGB
model_xgb <- hirem::fit(model_xgb,
                        weights = weight,
                        weight.var = 'dev.year',
                        balance.var = 'dev.year',
                        settlement = paste0('settlement ~ ', paste0(covariates_xgb, collapse = ' + ')),
                        payment = paste0('payment ~ ', paste0(c(covariates_xgb, 'settlement'), collapse = ' + ')),
                        recovery = paste0('recovery ~ ', paste0(c(covariates_xgb, 'settlement'), collapse = ' + ')),
                        size.pay = paste0('size.pay ~ ', paste0(c(covariates_xgb,'settlement'), collapse = ' + ')),
                        q.size.recov = paste0('q.size.recov ~ ', paste0(c(covariates_xgb,'settlement'), collapse = ' + ')))
```

```{r}
# Variable importance
xgb.importance(colnames(model_xgb$layers$settlement$fit), model = model_xgb$layers$settlement$fit)

```

```{r}
# Variable importance
xgb.importance(colnames(model_xgb$layers$payment$fit), model = model_xgb$layers$payment$fit)

```

```{r}
# Variable importance
xgb.importance(colnames(model_xgb$layers$recovery$fit), model = model_xgb$layers$recovery$fit)

```

```{r}
# Variable importance
xgb.importance(colnames(model_xgb$layers$size.pay$fit), model = model_xgb$layers$size.pay$fit)

```

```{r}
# Variable importance
xgb.importance(colnames(model_xgb$layers$q.size.recov$fit), model = model_xgb$layers$q.size.recov$fit)

```

### Hierarchical Hybrid DNN/GLM

We will use the gamma deviance as custom loss function and metric in `keras`:

```{r, message=FALSE}
gamma_deviance_keras <- function(yobs, yhat) {
  K <- backend()
  2*K$mean((yobs-yhat)/yhat - K$log(yobs/yhat))
}

metric_gamma_deviance_keras <- keras::custom_metric("gamma_deviance_keras", function(yobs, yhat) {
  gamma_deviance_keras(yobs, yhat)
})
```

We first define the model specifications of each layer. We only train the reserving model on the claim updates that are still open at the beginning of each year and are observed on the evaluation date (`calendar.year <= 10`). 

```{r message=FALSE, warning=FALSE}

# Model specifications
model_dnn <-  hirem(reserving_data) %>%
  split_data(observed = function(df) df %>% filter(calendar.year <= 10, open == 1)) %>%
  layer_dnn('settlement', distribution = 'bernoulli',
            bias_regularization = T, 
            hidden = c(10,10,10),
            loss = 'binary_crossentropy',
            metrics = 'binary_crossentropy',
            optimizer = optimizer_nadam(),
            validation_split = .3,
            activation.output = 'sigmoid',
            family_for_init = binomial(), # default link = logit
            epochs = 100,
            batch_size = 1000,
            monitor = 'val_binary_crossentropy',
            patience = 20) %>%
  layer_dnn('payment', distribution = 'bernoulli',
            bias_regularization = T,
            hidden = c(10,10,10),
            loss = 'binary_crossentropy',
            metrics = 'binary_crossentropy',
            optimizer = optimizer_nadam(),
            validation_split = .3,
            activation.output = 'sigmoid',
            family_for_init = binomial(), # default link = logit
            epochs = 100,
            batch_size = 1000,
            monitor = 'val_binary_crossentropy',
            patience = 20) %>%
  layer_dnn('recovery', distribution = 'bernoulli',
            bias_regularization = T,
            hidden = c(10,10,10),
            loss = 'binary_crossentropy',
            metrics = 'binary_crossentropy',
            optimizer = optimizer_nadam(),
            validation_split = .3,
            activation.output = 'sigmoid',
            family_for_init = binomial(), # default link = logit
            epochs = 100,
            batch_size = 1000,
            monitor = 'val_binary_crossentropy',
            patience = 20) %>%
  layer_dnn('size.pay', distribution = 'gamma',
            bias_regularization = T, 
            hidden = c(10,10,20),
            loss = gamma_deviance_keras,
            metrics = metric_gamma_deviance_keras,
            optimizer = optimizer_nadam(),
            validation_split = .3,
            activation.output = 'exponential',
            family_for_init = Gamma(link=log), # default link = inverse
            epochs = 100,
            batch_size = 1000,
            monitor = 'val_gamma_deviance_keras',
            patience = 20,
            filter = function(data){data$payment == 1}) %>%
  layer_glm(name = 'q.size.recov', 'family' = Gamma(link = 'log'), 
            filter = function(x){x$recovery == 1})
```
We now fit the hierarchical DNN on the observed portfolio of simulated claims.

```{r message=FALSE, warning=FALSE}
# Covariates
covariates_dnn <- c('type', 'dev.year.fact', 'rep.month', 'rep.year.fact', 'calendar.year', 'age.fact')

# Fitting the hierarchical MLP
model_dnn <- hirem::fit(model_dnn,
                        balance.var = 'dev.year',
                        settlement = paste0('settlement ~ ', paste0(covariates_dnn, collapse = ' + ')),
                        payment = paste0('payment ~ ', paste0(c(covariates_dnn, 'settlement'), collapse = ' + ')),
                        recovery = paste0('recovery ~ ', paste0(c(covariates_dnn, 'settlement'), collapse = ' + ')),
                        size.pay = paste0('size.pay ~ ', paste0(c(covariates_dnn,'settlement'), collapse = ' + ')),
                        q.size.recov = formula_size_recov_glm)
```

We can obtain the cross-validation results as follows:
```{r}
if(!is.null(model_dnn$layers$settlement$hyper_grid)) model_dnn$layers$settlement$hyper_grid
if(!is.null(model_dnn$layers$payment$hyper_grid)) model_dnn$layers$payment$hyper_grid
if(!is.null(model_dnn$layers$size$hyper_grid)) model_dnn$layers$size.pay$hyper_grid
```

We show the model retained:
```{r}
if(!is.null(model_dnn$layers$settlement$fit.biased)) model_dnn$layers$settlement$fit.biased
if(!is.null(model_dnn$layers$payment$fit.biased)) model_dnn$layers$payment$fit.biased
if(!is.null(model_dnn$layers$size$fit.biased)) model_dnn$layers$size.pay$fit.biased
```

### Hierarchical Hybrid DNN/GBM

We first define the model specifications of each layer. We only train the reserving model on the claim updates that are still open at the beginning of each year and are observed on the evaluation date (`calendar.year <= 10`). 

```{r message=FALSE, warning=FALSE}

# Parameters
gbm_param_size_recov   <- list('n.trees' = 700, 'interaction.depth' = 1, 'shrinkage' = 0.05)

# Model specifications
model_dnn2 <-  hirem(reserving_data) %>%
  split_data(observed = function(df) df %>% filter(calendar.year <= 10, open == 1)) %>%
  layer_dnn('settlement', distribution = 'bernoulli',
            bias_regularization = T, 
            hidden = c(10,10,10),
            loss = 'binary_crossentropy',
            metrics = 'binary_crossentropy',
            optimizer = optimizer_nadam(),
            validation_split = .3,
            activation.output = 'sigmoid',
            family_for_init = binomial(), # default link = logit
            epochs = 100,
            batch_size = 1000,
            monitor = 'val_binary_crossentropy',
            patience = 20) %>%
  layer_dnn('payment', distribution = 'bernoulli',
            bias_regularization = T,
            hidden = c(10,10,10),
            loss = 'binary_crossentropy',
            metrics = 'binary_crossentropy',
            optimizer = optimizer_nadam(),
            validation_split = .3,
            activation.output = 'sigmoid',
            family_for_init = binomial(), # default link = logit
            epochs = 100,
            batch_size = 1000,
            monitor = 'val_binary_crossentropy',
            patience = 20) %>%
  layer_dnn('recovery', distribution = 'bernoulli',
            bias_regularization = T,
            hidden = c(10,10,10),
            loss = 'binary_crossentropy',
            metrics = 'binary_crossentropy',
            optimizer = optimizer_nadam(),
            validation_split = .3,
            activation.output = 'sigmoid',
            family_for_init = binomial(), # default link = logit
            epochs = 100,
            batch_size = 1000,
            monitor = 'val_binary_crossentropy',
            patience = 20) %>%
  layer_dnn('size.pay', distribution = 'gamma',
            bias_regularization = T, 
            hidden = c(10,10,20),
            loss = gamma_deviance_keras,
            metrics = metric_gamma_deviance_keras,
            optimizer = optimizer_nadam(),
            validation_split = .3,
            activation.output = 'exponential',
            family_for_init = Gamma(link=log), # default link = inverse
            epochs = 100,
            batch_size = 1000,
            monitor = 'val_gamma_deviance_keras',
            patience = 20,
            filter = function(data){data$payment == 1}) %>%
  layer_gbm('q.size.recov', distribution = 'gamma', bag.fraction = 0.75, n.minobsinnode = 100,
            n.trees = gbm_param_size_recov$n.trees, interaction.depth = gbm_param_size_recov$interaction.depth,
            shrinkage = gbm_param_size_recov$shrinkage, select_trees = 'last',
            filter = function(data){data$recovery == 1})
```
We now fit the hierarchical DNN on the observed portfolio of simulated claims.

```{r message=FALSE, warning=FALSE}
# Covariates
covariates_dnn <- c('type', 'dev.year.fact', 'rep.month', 'rep.year.fact', 'calendar.year', 'age.fact')
covariates_gbm <- c('type', 'dev.year.fact', 'rep.month', 'rep.year.fact', 'age.fact', 'calendar.year')

# Fitting the hierarchical MLP
model_dnn2 <- hirem::fit(model_dnn2,
                        balance.var = 'dev.year',
                        settlement = paste0('settlement ~ ', paste0(covariates_dnn, collapse = ' + ')),
                        payment = paste0('payment ~ ', paste0(c(covariates_dnn, 'settlement'), collapse = ' + ')),
                        recovery = paste0('recovery ~ ', paste0(c(covariates_dnn, 'settlement'), collapse = ' + ')),
                        size.pay = paste0('size.pay ~ ', paste0(c(covariates_dnn,'settlement'), collapse = ' + ')),
                        q.size.recov = paste0('q.size.recov ~ 1 + ', paste0(c(covariates_gbm,'settlement'), collapse = ' + ')))
```

### Hierarchical Hybrid CANN/GLM

We first define the model specifications of each layer. We only train the reserving model on the claim updates that are still open at the beginning of each year and are observed on the evaluation date (`calendar.year <= 10`).

```{r message=FALSE, warning=FALSE}
# Model specifications
model_cann <-  hirem(reserving_data) %>%
  split_data(observed = function(df) df %>% filter(calendar.year <= 10, open == 1)) %>%
  layer_cann('settlement', distribution = 'bernoulli',
             hidden = c(10,20,10),
             formula.glm = formula_settle_glm, 
             bias_regularization = T,
             family_for_glm = binomial(),
             loss = 'binary_crossentropy',
             metrics = 'binary_crossentropy',
             optimizer = optimizer_nadam(),
             validation_split = .3,
             activation.output = 'linear',
             activation.output.cann = 'sigmoid',
             fixed.cann = T,
             monitor = 'val_binary_crossentropy',
             patience = 20,
             epochs = 100,
             batch_size = 1000) %>%
  layer_cann('payment', distribution = 'bernoulli',
             hidden = c(10,10,20),
             formula.glm = formula_pay_glm, 
             bias_regularization = T,
             family_for_glm = binomial(),
             loss = 'binary_crossentropy',
             metrics = 'binary_crossentropy',
             optimizer = optimizer_nadam(),
             validation_split = .3,
             activation.output = 'linear',
             activation.output.cann = 'sigmoid',
             fixed.cann = T,
             monitor = 'val_binary_crossentropy',
             patience = 20,
             epochs = 100,
             batch_size = 1000) %>%
  layer_cann('recovery', distribution = 'bernoulli',
             hidden = c(10,10,20),
             formula.glm = formula_recov_glm, 
             bias_regularization = T,
             family_for_glm = binomial(),
             loss = 'binary_crossentropy',
             metrics = 'binary_crossentropy',
             optimizer = optimizer_nadam(),
             validation_split = .3,
             activation.output = 'linear',
             activation.output.cann = 'sigmoid',
             fixed.cann = T,
             monitor = 'val_binary_crossentropy',
             patience = 20,
             epochs = 100,
             batch_size = 1000) %>%
  layer_cann('size.pay', distribution = 'gamma',
             hidden = c(30,20,10),
             formula.glm = formula_size_pay_glm, 
             bias_regularization = T,
             family_for_glm = Gamma(link=log),
             loss = gamma_deviance_keras,
             metrics = metric_gamma_deviance_keras,
             optimizer = optimizer_nadam(),
             validation_split = .3,
             activation.output = 'linear',
             activation.output.cann = 'exponential',
             fixed.cann = T,
             monitor = 'val_gamma_deviance_keras',
             patience = 20,
             epochs = 100, 
             batch_size = 1000,
             filter = function(data){data$payment == 1}) %>%
  layer_glm(name = 'q.size.recov', 'family' = Gamma(link = 'log'), 
             filter = function(x){x$recovery == 1})
```

We now fit the hierarchical CANN on the observed portfolio of simulated claims. Notice that `dev.year.fact` is removed from the list of covariates to prevent an issue in the calibration process (loss function is `nan`).

```{r}
#Covariates
covariates_cann <- c('type', 'dev.year.fact', 'rep.month', 'rep.year.fact', 'calendar.year', 'age.fact')

model_cann <- hirem::fit(model_cann,
                 balance.var = 'dev.year',
                 settlement = paste0('settlement ~ ', paste0(covariates_cann, collapse = ' + ')),
                 payment = paste0('payment ~ ', paste0(c(covariates_cann, 'settlement'), collapse = ' + ')),
                 recovery = paste0('recovery ~ ', paste0(c(covariates_cann, 'settlement'), collapse = ' + ')),
                 size.pay = paste0('size.pay ~ ', paste0(c(covariates_cann,'settlement'), collapse = ' + ')),
                 q.size.recov = formula_size_recov_glm)
```

We show the model retained:
```{r}
if(!is.null(model_cann$layers$settlement$fit.biased)) model_cann$layers$settlement$fit.biased
if(!is.null(model_cann$layers$payment$fit.biased)) model_cann$layers$payment$fit.biased
if(!is.null(model_cann$layers$size$fit.biased)) model_cann$layers$size$fit.biased
```

### Hierarchical Hybrid CANN/GBM

We first define the model specifications of each layer. We only train the reserving model on the claim updates that are still open at the beginning of each year and are observed on the evaluation date (`calendar.year <= 10`).

```{r message=FALSE, warning=FALSE}
# Parameters
gbm_param_size_recov   <- list('n.trees' = 700, 'interaction.depth' = 1, 'shrinkage' = 0.05)

# Model specifications
model_cann2 <-  hirem(reserving_data) %>%
  split_data(observed = function(df) df %>% filter(calendar.year <= 10, open == 1)) %>%
  layer_cann('settlement', distribution = 'bernoulli',
             hidden = c(10,20,10),
             formula.glm = formula_settle_glm, 
             bias_regularization = T,
             family_for_glm = binomial(),
             loss = 'binary_crossentropy',
             metrics = 'binary_crossentropy',
             optimizer = optimizer_nadam(),
             validation_split = .3,
             activation.output = 'linear',
             activation.output.cann = 'sigmoid',
             fixed.cann = T,
             monitor = 'val_binary_crossentropy',
             patience = 20,
             epochs = 100,
             batch_size = 1000) %>%
  layer_cann('payment', distribution = 'bernoulli',
             hidden = c(10,10,20),
             formula.glm = formula_pay_glm, 
             bias_regularization = T,
             family_for_glm = binomial(),
             loss = 'binary_crossentropy',
             metrics = 'binary_crossentropy',
             optimizer = optimizer_nadam(),
             validation_split = .3,
             activation.output = 'linear',
             activation.output.cann = 'sigmoid',
             fixed.cann = T,
             monitor = 'val_binary_crossentropy',
             patience = 20,
             epochs = 100,
             batch_size = 1000) %>%
  layer_cann('recovery', distribution = 'bernoulli',
             hidden = c(10,10,20),
             formula.glm = formula_recov_glm, 
             bias_regularization = T,
             family_for_glm = binomial(),
             loss = 'binary_crossentropy',
             metrics = 'binary_crossentropy',
             optimizer = optimizer_nadam(),
             validation_split = .3,
             activation.output = 'linear',
             activation.output.cann = 'sigmoid',
             fixed.cann = T,
             monitor = 'val_binary_crossentropy',
             patience = 20,
             epochs = 100,
             batch_size = 1000) %>%
  layer_cann('size.pay', distribution = 'gamma',
             hidden = c(30,20,10),
             formula.glm = formula_size_pay_glm, 
             bias_regularization = T,
             family_for_glm = Gamma(link=log),
             loss = gamma_deviance_keras,
             metrics = metric_gamma_deviance_keras,
             optimizer = optimizer_nadam(),
             validation_split = .3,
             activation.output = 'linear',
             activation.output.cann = 'exponential',
             fixed.cann = T,
             monitor = 'val_gamma_deviance_keras',
             patience = 20,
             epochs = 100, 
             batch_size = 1000,
             filter = function(data){data$payment == 1}) %>%
  layer_gbm('q.size.recov', distribution = 'gamma', bag.fraction = 0.75, n.minobsinnode = 100,
            n.trees = gbm_param_size_recov$n.trees, interaction.depth = gbm_param_size_recov$interaction.depth,
            shrinkage = gbm_param_size_recov$shrinkage, select_trees = 'last',
            filter = function(data){data$recovery == 1})
```

We now fit the hierarchical CANN on the observed portfolio of simulated claims. Notice that `dev.year.fact` is removed from the list of covariates to prevent an issue in the calibration process (loss function is `nan`).

```{r}
#Covariates
covariates_cann <- c('type', 'dev.year.fact', 'rep.month', 'rep.year.fact', 'calendar.year', 'age.fact')
covariates_gbm <- c('type', 'dev.year.fact', 'rep.month', 'rep.year.fact', 'age.fact', 'calendar.year')

model_cann2 <- hirem::fit(model_cann2,
                 balance.var = 'dev.year',
                 settlement = paste0('settlement ~ ', paste0(covariates_cann, collapse = ' + ')),
                 payment = paste0('payment ~ ', paste0(c(covariates_cann, 'settlement'), collapse = ' + ')),
                 recovery = paste0('recovery ~ ', paste0(c(covariates_cann, 'settlement'), collapse = ' + ')),
                 size.pay = paste0('size.pay ~ ', paste0(c(covariates_cann,'settlement'), collapse = ' + ')),
                 q.size.recov = paste0('size.recov ~ 1 + ', paste0(c(covariates_gbm,'settlement'), collapse = ' + ')))
```

## Predicting the future development of claims in the hierarchical reserving models
In the next step, we predict the future development of claims. We do this by simulating 50 different paths for each open claim in the year of the evaluation date and by averaging the results afterwards. In this prediction strategy, the hierarchical structure of the reserving model is preserved and the development of claims are simulated in chronological order.

We define an update function applied to the data for the simulation of each subsequent development year. 

```{r}
# Update function
update <- function(data) {
  data$dev.year <- data$dev.year + 1
  data$dev.year.fact <- factor(data$dev.year, levels = 1:10)
  
  data$calendar.year <- data$calendar.year + 1
  
  data$monthDev12[data$dev.year > 3] <- 'dev.year.over.3'
  data$devYearMonth <- factor(paste(data$dev.year, data$monthDev12, sep = '-'))

  data
}

model_glm <- register_updater(model_glm, update)
model_gbm <- register_updater(model_gbm, update)
model_xgb <- register_updater(model_xgb, update)
model_dnn <- register_updater(model_dnn, update)
model_dnn2 <- register_updater(model_dnn2, update)
model_cann <- register_updater(model_cann, update)
model_cann2 <- register_updater(model_cann2, update)
```

Next, we apply the actual simulation of the development of claims over time beyond the observation window of those claims that are still open at the evaluation date (calendar year 11). Moreover, we apply the balance correction as explained in @hirempaper.

```{r}
nsim <- 50 # Instead of 100 (original value), to avoid memory issues

simul_glm <- simulate(model_glm,
                      nsim = nsim,
                      filter = function(data){dplyr::filter(data, dev.year <= 10, settlement == 0)},
                      data = model_glm$data_observed %>% dplyr::filter(calendar.year == 10),
                      balance.correction = TRUE)

simul_gbm <- simulate(model_gbm,
                      nsim = nsim,
                      filter = function(data){dplyr::filter(data, dev.year <= 10, settlement == 0)},
                      data = model_gbm$data_observed %>% dplyr::filter(calendar.year == 10),
                      balance.correction = TRUE)

simul_xgb <- simulate(model_xgb,
                      nsim = nsim,
                      filter = function(data){dplyr::filter(data, dev.year <= 10, settlement == 0)},
                      data = model_xgb$data_observed %>% dplyr::filter(calendar.year == 10),
                      balance.correction = TRUE)

simul_dnn <- simulate(model_dnn,
                      nsim = nsim,
                      filter = function(data){dplyr::filter(data, dev.year <= 10, settlement == 0)},
                      data = model_dnn$data_observed %>% dplyr::filter(calendar.year == 10),
                      balance.correction = TRUE)

simul_dnn2 <- simulate(model_dnn2,
                      nsim = nsim,
                      filter = function(data){dplyr::filter(data, dev.year <= 10, settlement == 0)},
                      data = model_dnn2$data_observed %>% dplyr::filter(calendar.year == 10),
                      balance.correction = TRUE)

simul_cann <- simulate(model_cann,
                      nsim = nsim,
                      filter = function(data){dplyr::filter(data, dev.year <= 10, settlement == 0)},
                      data = model_cann$data_observed %>% dplyr::filter(calendar.year == 10),
                      balance.correction = TRUE)

simul_cann2 <- simulate(model_cann2,
                      nsim = nsim,
                      filter = function(data){dplyr::filter(data, dev.year <= 10, settlement == 0)},
                      data = model_cann2$data_observed %>% dplyr::filter(calendar.year == 10),
                      balance.correction = TRUE)

```

## Chain-ladder model
For comparison, we apply the classical chain ladder model to predict the total number of open claims, the total number of payments and the total payment sizes outside the observation window.

We first construct the incremental run-off triangles for the number of open claims, the number of payments and the total payment sizes in each reporting year and development year since reporting in the observed portfolio. We know the number of open claims in the year following the evaluation date since we have information whether or not a claim settles in the year of settlement. 

```{r}
# Incremental run-off triangles
triangle_open    <- construct_triangle(data = observed_data %>% filter(open == 1), group.var1 = 'rep.year', 
                                       group.var2 = 'dev.year', value = 'open', cumulative = FALSE)
triangle_payment <- construct_triangle(data = observed_data, group.var1 = 'rep.year',
                                       group.var2 = 'dev.year', value = 'payment', cumulative = FALSE)
triangle_size    <- construct_triangle(data = observed_data, group.var1 = 'rep.year',
                                       group.var2 = 'dev.year', value = 'size', cumulative = FALSE)

# Number of open claims in the year following the evaluation date
settle.evalyear <- observed_data %>% 
  filter(open == 1, calendar.year == 10) %>%
  group_by(rep.year, dev.year) %>%
  summarise(settlement = sum(settlement))

# The number of open claims in the year after the evaluation date
triangle_open[row(triangle_open) + col(triangle_open) == 12] <- 
  (triangle_open[row(triangle_open) + col(triangle_open) == 11] - rev(settle.evalyear$settlement))[1:9]
```

We then apply (a special version of) the chain-ladder model on the incremental run-off triangle for the number of open claims. For the number of payments and the payment sizes, we use the classical chain ladder model applied on the cumulative run-off triangles.

```{r}
# Chain ladder predictions
cl_open <- chainLadder_open(triangle_open)
cl_pay  <- chainLadder(triangle_payment, is_cumulatif = FALSE)
cl_size <- chainLadder(triangle_size, is_cumulatif = FALSE)
```

## Evaluating the predictive performance of the chain-ladder model and the hierarchical GLM and GBM

First, we compare the prediction for the total number of open claims in the test set.

```{r}
nsim <- 50

# Predictions
obs_open_total <- prediction_data %>% filter(calendar.year != 11) %>% summarise(Total = sum(open)) %>% pull(Total)
cl_open_total  <- sum(cl_open)
glm_open_total <- simul_glm %>% filter(calendar.year != 11) %>% summarise(Total = sum(open)/nsim) %>% pull(Total)
gbm_open_total <- simul_gbm %>% filter(calendar.year != 11) %>% summarise(Total = sum(open)/nsim) %>% pull(Total)
xgb_open_total <- simul_xgb %>% filter(calendar.year != 11) %>% summarise(Total = sum(open)/nsim) %>% pull(Total)
dnn_open_total <- simul_dnn %>% filter(calendar.year != 11) %>% summarise(Total = sum(open)/nsim) %>% pull(Total)
dnn2_open_total <- simul_dnn2 %>% filter(calendar.year != 11) %>% summarise(Total = sum(open)/nsim) %>% pull(Total)
cann_open_total <- simul_cann %>% filter(calendar.year != 11) %>% summarise(Total = sum(open)/nsim) %>% pull(Total)
cann2_open_total <- simul_cann2 %>% filter(calendar.year != 11) %>% summarise(Total = sum(open)/nsim) %>% pull(Total)

# Print Results
c('Actual' =  obs_open_total, 'Chain-Ladder' = cl_open_total, 'Hierarchical GLM' = glm_open_total, 'Hierarchical GBM' = gbm_open_total, 'Hierarchical XGB' = xgb_open_total, 'Hierarchical DNN/GLM' = dnn_open_total, 'Hierarchical DNN/GBM' = dnn2_open_total, 'Hierarchical CANN/GLM' = cann_open_total, 'Hierarchical CANN/GBM' = cann2_open_total)
```

Second, we compare the prediction for the total number of payments in the prediction data set.

```{r}
# Predictions
obs_pay_total <- prediction_data %>% summarise(Total = sum(payment)) %>% pull(Total)
cl_pay_total  <- sum(cl_pay)
glm_pay_total <- simul_glm %>% summarise(Total = sum(payment)/nsim) %>% pull(Total)
gbm_pay_total <- simul_gbm %>% summarise(Total = sum(payment)/nsim) %>% pull(Total)
xgb_pay_total <- simul_xgb %>% summarise(Total = sum(payment)/nsim) %>% pull(Total)
dnn_pay_total <- simul_dnn %>% summarise(Total = sum(payment)/nsim) %>% pull(Total)
cann_pay_total <- simul_cann %>% summarise(Total = sum(payment)/nsim) %>% pull(Total)
dnn2_pay_total <- simul_dnn2 %>% summarise(Total = sum(payment)/nsim) %>% pull(Total)
cann2_pay_total <- simul_cann2 %>% summarise(Total = sum(payment)/nsim) %>% pull(Total)

# Print Results
c('Actual' = obs_pay_total, 'Chain-Ladder' = cl_pay_total, 'Hierarchical GLM' = glm_pay_total, 'Hierarchical GBM' = gbm_pay_total, 'Hierarchical XGB' = xgb_pay_total, 'Hierarchical DNN/GLM' = dnn_pay_total, 'Hierarchical DNN/GBM' = dnn2_pay_total, 'Hierarchical CANN/GLM' = cann_pay_total, 'Hierarchical CANN/GBM' = cann2_pay_total)
```

Third, we compare the prediction for the total number of recoveries in the prediction data set.

```{r}
# Predictions
obs_recov_total <- prediction_data %>% summarise(Total = sum(recovery)) %>% pull(Total)
#cl_pay_total  <- sum(cl_pay)
glm_recov_total <- simul_glm %>% summarise(Total = sum(recovery)/nsim) %>% pull(Total)
gbm_recov_total <- simul_gbm %>% summarise(Total = sum(recovery)/nsim) %>% pull(Total)
xgb_recov_total <- simul_xgb %>% summarise(Total = sum(recovery)/nsim) %>% pull(Total)
dnn_recov_total <- simul_dnn %>% summarise(Total = sum(recovery)/nsim) %>% pull(Total)
cann_recov_total <- simul_cann %>% summarise(Total = sum(recovery)/nsim) %>% pull(Total)
dnn2_recov_total <- simul_dnn2 %>% summarise(Total = sum(recovery)/nsim) %>% pull(Total)
cann2_recov_total <- simul_cann2 %>% summarise(Total = sum(recovery)/nsim) %>% pull(Total)

# Print Results
c('Actual' = obs_recov_total, 'Hierarchical GLM' = glm_recov_total, 'Hierarchical GBM' = gbm_recov_total, 'Hierarchical XGB' = xgb_recov_total, 'Hierarchical DNN/GLM' = dnn_recov_total, 'Hierarchical DNN/GBM' = dnn2_recov_total, 'Hierarchical CANN/GLM' = cann_recov_total, 'Hierarchical CANN/GBM' = cann2_recov_total)
```

Fourth, we compare the prediction for the total number of payment sizes in the prediction set.

```{r}
# Predictions
obs_size_pay_total <- prediction_data %>% summarise(Total = sum(size.pay)) %>% pull(Total)
cl_size_pay_total  <- sum(cl_size)
glm_size_pay_total <- simul_glm %>% summarise(Total = sum(size.pay)/nsim) %>% pull(Total)
gbm_size_pay_total <- simul_gbm %>% summarise(Total = sum(size.pay)/nsim) %>% pull(Total)
xgb_size_pay_total <- simul_xgb %>% summarise(Total = sum(size.pay)/nsim) %>% pull(Total)
dnn_size_pay_total <- simul_dnn %>% summarise(Total = sum(size.pay)/nsim) %>% pull(Total)
cann_size_pay_total <- simul_cann %>% summarise(Total = sum(size.pay)/nsim) %>% pull(Total)
dnn2_size_pay_total <- simul_dnn2 %>% summarise(Total = sum(size.pay)/nsim) %>% pull(Total)
cann2_size_pay_total <- simul_cann2 %>% summarise(Total = sum(size.pay)/nsim) %>% pull(Total)

# Print Results
c('Actual' = obs_size_pay_total, 'Chain-Ladder' = cl_size_pay_total, 'Hierarchical GLM' = glm_size_pay_total, 'Hierarchical GBM' = gbm_size_pay_total, 'Hierarchical XGB' = xgb_size_pay_total, 'Hierarchical DNN/GLM' = dnn_size_pay_total, 'Hierarchical DNN/GBM' = dnn2_size_pay_total, 'Hierarchical CANN/GLM' = cann_size_pay_total, 'Hierarchical CANN/GBM' = cann2_size_pay_total)
```

Fifth, we compare the prediction for the total amount of recovery sizes in the prediction set.

```{r}
update_simul <- function(simul) {
  
  ultimate_simul <- simul %>% 
    dplyr::select(-total.size.pay, -size.recov) %>%
    dplyr::group_by(simulation, Id) %>%
    dplyr::summarize(total.size.pay = sum(size.pay))

  simul <- simul %>% dplyr::select(-total.size.pay, -size.recov)

  simul <- merge(simul, ultimate_simul, by=c("simulation", "Id"), all.x = T)

  simul <- simul %>% 
    mutate(p.size.recov = q.size.recov / (1+q.size.recov), 
           size.recov = p.size.recov * total.size.pay)
  
  return(simul)
  
}

simul_glm <- update_simul(simul_glm)
simul_gbm <- update_simul(simul_gbm)
simul_xgb <- update_simul(simul_xgb)
simul_dnn <- update_simul(simul_dnn)
simul_dnn2 <- update_simul(simul_dnn2)
simul_cann <- update_simul(simul_cann)
simul_cann2 <- update_simul(simul_cann2)

# Predictions
obs_size_recov_total <- prediction_data %>% summarise(Total = sum(size.recov)) %>% pull(Total)
#cl_size_recov_total  <- sum(cl_size)
glm_size_recov_total <- simul_glm %>% summarise(Total = sum(size.recov)/nsim) %>% pull(Total)
gbm_size_recov_total <- simul_gbm %>% summarise(Total = sum(size.recov)/nsim) %>% pull(Total)
xgb_size_recov_total <- simul_xgb %>% summarise(Total = sum(size.recov)/nsim) %>% pull(Total)
dnn_size_recov_total <- simul_dnn %>% summarise(Total = sum(size.recov)/nsim) %>% pull(Total)
cann_size_recov_total <- simul_cann %>% summarise(Total = sum(size.recov)/nsim) %>% pull(Total)
dnn2_size_recov_total <- simul_dnn2 %>% summarise(Total = sum(size.recov)/nsim) %>% pull(Total)
cann2_size_recov_total <- simul_cann2 %>% summarise(Total = sum(size.recov)/nsim) %>% pull(Total)

# Print Results
c('Actual' = obs_size_recov_total, 'Hierarchical GLM' = glm_size_recov_total, 'Hierarchical GBM' = gbm_size_recov_total, 'Hierarchical XGB' = xgb_size_recov_total, 'Hierarchical DNN/GLM' = dnn_size_recov_total, 'Hierarchical DNN/GBM' = dnn2_size_recov_total, 'Hierarchical CANN/GLM' = cann_size_recov_total, 'Hierarchical CANN/GBM' = cann2_size_recov_total)
```

Comparing the empirical cumulative distribution function obtained from the prediction dataset with the one obtained from the simulated data:
```{r}
Fn <- prediction_data %>% filter(size.recov > 0) %>% pull(size.recov)
ecdf_pred <- ecdf(Fn)
plot(ecdf_pred, do.points = F, col.hor = "red")
ecdf_glm <- ecdf(simul_glm %>% filter(size.recov > 0) %>% pull(size.recov))
lines(ecdf_glm, do.points = F, col.hor = "green")
ecdf_gbm <- ecdf(simul_gbm %>% filter(size.recov > 0) %>% pull(size.recov))
lines(ecdf_gbm, do.points = F, col.hor = "blue")
ecdf_xgb <- ecdf(simul_xgb %>% filter(size.recov > 0) %>% pull(size.recov))
lines(ecdf_xgb, do.points = F, col.hor = "orange")
```
```{r}
Fn <- prediction_data %>% filter(size.recov > 0) %>% pull(size.recov)
ecdf_pred <- ecdf(Fn)
plot(ecdf_pred, do.points = F, col.hor = "red")
ecdf_dnn <- ecdf(simul_dnn %>% filter(size.recov > 0) %>% pull(size.recov))
lines(ecdf_dnn, do.points = F, col.hor = "green")
ecdf_dnn2 <- ecdf(simul_dnn2 %>% filter(size.recov > 0) %>% pull(size.recov))
lines(ecdf_dnn2, do.points = F, col.hor = "blue")
```
```{r}
Fn <- prediction_data %>% filter(size.recov > 0) %>% pull(size.recov)
ecdf_pred <- ecdf(Fn)
plot(ecdf_pred, do.points = F, col.hor = "red")
ecdf_cann <- ecdf(simul_cann %>% filter(size.recov > 0) %>% pull(size.recov))
lines(ecdf_cann, do.points = F, col.hor = "green")
ecdf_cann2 <- ecdf(simul_cann2 %>% filter(size.recov > 0) %>% pull(size.recov))
lines(ecdf_cann2, do.points = F, col.hor = "blue")
```


## References

<div id="refs"></div>
