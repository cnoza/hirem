% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer.R
\name{layer_cann}
\alias{layer_cann}
\title{Layer estimated using a Combined Actuarial Neural Network (CANN)}
\usage{
layer_cann(
  obj,
  name,
  distribution = "gaussian",
  family_for_glm = Gamma(link = log),
  use_bias = TRUE,
  formula.glm = NULL,
  hidden = NULL,
  dropout.hidden = NULL,
  step_log = FALSE,
  step_normalize = FALSE,
  use_embedding = FALSE,
  bias_regularization = NULL,
  activation.hidden = NULL,
  activation.output = "linear",
  activation.output.cann = "linear",
  embedding_var = c(),
  embedding_var.glm = c(),
  fixed.cann = TRUE,
  batch_normalization = FALSE,
  monitor = "loss",
  patience = 20,
  verbose = 0,
  shuffle = TRUE,
  loss = "mse",
  optimizer = "nadam",
  epochs = 20,
  batch_size = 1000,
  validation_split = 0.2,
  metrics = NULL,
  nfolds = 5,
  bayesOpt = FALSE,
  bayesOpt_min = FALSE,
  bayesOpt_iters_n = 3,
  bayesOpt_bounds = NULL,
  bayesOpt_initPoints = 4,
  bayesOpt_step = 1,
  filter = NULL,
  transformation = NULL,
  gridsearch_cv = F,
  gridsearch_cv.min = T,
  hyper_grid = NULL,
  random_trials = 0
)
}
\arguments{
\item{obj}{The hierarchical reserving model}

\item{name}{Character, name of the layer. This name should match the variable name in the data set}

\item{distribution}{The distribution used for the simulation. Default is gaussian,}

\item{family_for_glm}{The family used to estimate the GLM model. The coefficient estimates are then used to initialize the weights of the associated GLM neural network.
Default is Gamma(link = log).}

\item{hidden}{The hidden layer architecture of the Neural Network, passed to \code{keras}. Default is c(30,20,10).}

\item{dropout.hidden}{The dropout ratios for each hidden layer of the Neural Network, passed to \code{keras}. Default is 0.}

\item{step_log}{If TRUE, the logarithmic transformation is applied on the response variable through the use of a \code{recipe}.}

\item{step_normalize}{If TRUE, the \code{step_normalize} function is used to preprocess the input data through the use of a \code{recipe}.}

\item{activation.hidden}{The activation function for each hidden layer of the Neural Network, passed to \code{keras}. Default is tanh.}

\item{activation.output}{The activation function for the output layer of the Neural Network, passed to \code{keras}. Default is linear.}

\item{activation.output.cann}{The activation function for the output layer of the CANN, passed to \code{keras}. Default is linear.}

\item{fixed.cann}{If TRUE (default), the weights of the CANN's output layer are fixed and non trainable.}

\item{monitor}{The monitor argument passed to \code{keras}. Default is 'loss'.}

\item{patience}{The patience argument passed to \code{keras}. Default is 20.}

\item{loss}{The loss function argument passed to \code{keras}. Default is 'mse'.}

\item{optimizer}{The optimizer argument passed to \code{keras}. Default is 'nadam'.}

\item{epochs}{The epochs argument passed to \code{keras}. Default is 20.}

\item{batch_size}{The batch_size argument passed to \code{keras}. Default is 1000.}

\item{validation_split}{The validation_split argument passed to \code{keras}. Default is .2}

\item{metrics}{The metrics argument passed to \code{keras}.}

\item{filter}{Function with \itemize{
 \item input: Data set with same structure as the data passed to \code{hirem}
 \item output: TRUE/FALSE vector with same length as the number of rows in the input data set.\cr
       FALSE indicates that this layer is zero for the current record.
}}

\item{transformation}{Object of class \code{hirem_transformation} specifying the transformation
applied before modelling this layer.}
}
\description{
Adds a new layer to the hierarchical reserving model. This layer will be estimated using a CANN architecture,
see Schelldorfer, J., & Wuthrich, M. (2019). Nesting Classical Actuarial Models into Neural Networks. Applied Computing eJournal.
}
