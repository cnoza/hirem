% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer.R
\name{layer_mlp_keras}
\alias{layer_mlp_keras}
\title{Layer estimated using a multi-layer perceptron model with Keras}
\usage{
layer_mlp_keras(
  obj,
  name,
  distribution = "gaussian",
  use_bias = TRUE,
  ae.hidden = NULL,
  ae.activation.hidden = NULL,
  hidden = NULL,
  dropout.hidden = NULL,
  step_log = FALSE,
  step_normalize = FALSE,
  verbose = 0,
  activation.hidden = NULL,
  activation.output = "linear",
  batch_normalization = FALSE,
  loss = "mse",
  optimizer = "nadam",
  epochs = 20,
  batch_size = 1000,
  validation_split = 0.2,
  metrics = NULL,
  monitor = "loss",
  patience = 20,
  family_for_init = NULL,
  bayesOpt = FALSE,
  bayesOpt_min = FALSE,
  bayesOpt_iters_n = 3,
  bayesOpt_bounds = NULL,
  filter = NULL,
  transformation = NULL
)
}
\arguments{
\item{obj}{The hierarchical reserving model}

\item{name}{Character, name of the layer. This name should match the variable name in the data set}

\item{distribution}{The distribution used for the simulation. Default is gaussian.}

\item{use_bias}{Argument passed to \code{keras} in the definition of the MLP architecture.}

\item{ae.hidden}{The hidden layer architecture of a (stacked) autoencoder to be fitted on the input data.
If used, then the encoding part is used to compress the input data, before fitting the MLP model.}

\item{ae.activation.hidden}{The activation functions to be used in the hidden layer architecture of the (stacked) autoencoder.
If NULL, the linear activation function is used in all hidden layers.}

\item{hidden}{The hidden layer architecture passed to \code{keras}.}

\item{dropout.hidden}{The dropout ratios for each hidden layer passed to \code{keras}. Default is 0.}

\item{step_log}{If TRUE, the logarithmic transformation is applied on the response variable through the use of a \code{recipe}.}

\item{step_normalize}{If TRUE, the \code{step_normalize} function is used to preprocess the input data through the use of a \code{recipe}.}

\item{verbose}{The verbose argument passed to the \code{fit} function of \code{keras}. Default is 1.}

\item{activation.hidden}{The activation function for each hidden layer passed to \code{keras}. Default is Relu}

\item{activation.output}{The activation function for the output layer passed to \code{keras}. Default is Linear}

\item{batch_normalization}{If TRUE (default), apply the batch normalization between each hidden layer.}

\item{loss}{The loss function argument passed to \code{keras}. Default is 'mse'.}

\item{optimizer}{The optimizer argument passed to \code{keras}. Default is 'nadam'.}

\item{epochs}{The epochs argument passed to \code{keras}. Default is 20.}

\item{batch_size}{The batch_size argument passed to \code{keras}. Default is 1000.}

\item{validation_split}{The validation_split argument passed to \code{keras}. Default is .2}

\item{metrics}{The metrics argument passed to \code{keras}.}

\item{monitor}{The monitor argument passed to \code{keras}. Default is 'loss'.}

\item{patience}{The patience argument passed to \code{keras}. Default is 20.}

\item{family_for_init}{If not NULL, an homogenous GLM is estimated and the resulting coefficient estimate is used to initialize the bias weight in the output layer, which may improve convergence.
See Ferrario, Andrea and Ferrario, Andrea and Noll, Alexander and Wuthrich, Mario V., Insights from Inside Neural Networks (April 23, 2020). Available at SSRN: https://ssrn.com/abstract=3226852 or http://dx.doi.org/10.2139/ssrn.3226852}

\item{filter}{Function with \itemize{
 \item input: Data set with same structure as the data passed to \code{hirem}
 \item output: TRUE/FALSE vector with same length as the number of rows in the input data set.\cr
       FALSE indicates that this layer is zero for the current record.
}}

\item{transformation}{Object of class \code{hirem_transformation} specifying the transformation
applied before modelling this layer.}

\item{scale}{If TRUE (default), the data used for training is scaled.}
}
\description{
Adds a new layer to the hierarchical reserving model. This layer will be estimated using the \code{\link[keras]{keras}} package.
}
