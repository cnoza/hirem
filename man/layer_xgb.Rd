% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layer.R
\name{layer_xgb}
\alias{layer_xgb}
\title{Layer estimated using an extreme gradient boosting model}
\usage{
layer_xgb(
  obj,
  name,
  nrounds = 500,
  early_stopping_rounds = 50,
  verbose = F,
  booster = "gbtree",
  objective,
  eval_metric = "rmse",
  eta = 0.01,
  nthread = 1,
  subsample = 0.8,
  colsample_bynode = 0.8,
  max_depth = 2,
  min_child_weight = 10,
  gamma = 0,
  lambda = 0.01,
  alpha = 0.01,
  filter = NULL,
  transformation = NULL
)
}
\arguments{
\item{obj}{The hierarchical reserving model}

\item{name}{Character, name of the layer. This name should match the variable name in the data set}

\item{nrounds}{Max number of boosting iterations, passed to \code{\link[xgboost]{xgboost}}. Default is 100.}

\item{early_stopping_rounds}{Passed to \code{\link[xgboost]{xgboost}}. If NULL, the early stopping function is not triggered. If set to an integer k, training with a validation set will stop if the performance doesn't improve for k rounds. Default is 20.}

\item{verbose}{If 0, \code{\link[xgboost]{xgboost}} will stay silent. If 1, it will print information about performance. Default is 0.}

\item{booster}{Passed to \code{\link[xgboost]{xgboost}}. Which booster to use, can be gbtree or gblinear. Default is gbtree.}

\item{objective}{Specify the learning task and the corresponding learning objective, passed to \code{\link[xgboost]{xgboost}}.}

\item{eval_metric}{Evaluation metrics for validation data. Default is 'rmse'.}

\item{eta}{The learning rate passed to \code{\link[xgboost]{xgboost}}. Default is 0.01}

\item{nthread}{Number of parallel threads used to run \code{\link[xgboost]{xgboost}}. Default is 1.}

\item{subsample}{Subsample ratio of the training instance. Default is 0.8. Setting it to 0.8 means that \code{\link[xgboost]{xgboost}} randomly collected 80 percent of the data instances to grow trees and this will prevent overfitting.}

\item{colsample_bynode}{Subsample ratio of columns for each node (split). Passed to \code{\link[xgboost]{xgboost}}}

\item{max_depth}{Maximum depth of a tree, passed to \code{\link[xgboost]{xgboost}}. Default is 2.}

\item{min_child_weight}{Minimum sum of instance weight (hessian) needed in a child, passed to \code{\link[xgboost]{xgboost}}. Default is 10.}

\item{gamma}{Minimum loss reduction required to make a further partition on a leaf node of the tree, passed to \code{\link[xgboost]{xgboost}}. Default is 0.}

\item{lambda}{L2 regularization term on weights, passed to \code{\link[xgboost]{xgboost}}. Default is 0.01.}

\item{alpha}{L1 regularization term on weights, passed to \code{\link[xgboost]{xgboost}}. Default is 0.01.}

\item{filter}{Function with \itemize{
 \item input: Data set with same structure as the data passed to \code{\link{hirem}}
 \item output: TRUE/FALSE vector with same length as the number of rows in the input data set.\cr
       FALSE indicates that this layer is zero for the current record.
}}

\item{transformation}{Object of class \code{\link{hirem_transformation}} specifying the transformation
applied before modelling this layer.}
}
\description{
Adds a new layer to the hierarchical reserving model. This layer will be estimated using the \code{\link[xgboost]{xgboost}} package.
}
